{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distribution(df):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    for idx, label_type in enumerate(['clean_label', 'noisy_label']):\n",
    "        value_counts = df[label_type].value_counts().sort_index()\n",
    "        bars = axs[idx].bar(value_counts.index, value_counts.values)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axs[idx].text(bar.get_x() + bar.get_width()/2, height, str(height), \n",
    "                          ha='center', va='bottom', fontsize=10, color='black')\n",
    "        \n",
    "        axs[idx].set_title(label_type)\n",
    "        axs[idx].set_xticks(value_counts.index)\n",
    "    \n",
    "    plt.suptitle('Class Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corruption_plot(df):\n",
    "\n",
    "    clean_df = df[df['noisy_label'] == df['clean_label']]\n",
    "    corrupted_df = clean_df[clean_df['mean'].round().astype(int) != clean_df['noisy_label']]\n",
    "    # Data for the plot\n",
    "    categories = clean_df[\"clean_label\"].unique()  # X-axis labels\n",
    "    values1 =  [len(clean_df[clean_df[\"clean_label\"] == label]) for label in range(len(categories))] # First set of values\n",
    "    values2 = [len(corrupted_df[corrupted_df[\"clean_label\"] == label]) for label in range(len(categories))]  # Second set of values\n",
    "\n",
    "    # Define the width of each bar\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Generate positions for the bars\n",
    "    x = np.arange(len(categories))\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.bar(x - bar_width / 2, values1, width=bar_width, color='green', label='Clean Labels')\n",
    "    plt.bar(x + bar_width / 2, values2, width=bar_width, color='red', label='Corrupted Labels')\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Ground Truths')\n",
    "    plt.ylabel('Number of Data')\n",
    "    plt.title(f'Total Clean Labels = {len(clean_df)}, Total Corrupted Labels = {len(corrupted_df)}, Corruption Ratio = {round(len(corrupted_df)/len(clean_df), 4)}')\n",
    "    plt.xticks(ticks=x, labels=x)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the values on top of the bars\n",
    "    for i, (v1, v2) in enumerate(zip(values1, values2)):\n",
    "        plt.text(x[i] - bar_width / 2, v1 + 0.2, str(v1), ha='center')\n",
    "        plt.text(x[i] + bar_width / 2, v2 + 0.2, str(v2), ha='center')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_plot(df):\n",
    "    noisy_df = df[df['noisy_label'] != df['clean_label']]\n",
    "    corrected_df = noisy_df[noisy_df['mean'].round().astype(int) == noisy_df['clean_label']]\n",
    "    # Data for the plot\n",
    "    categories = noisy_df[\"clean_label\"].unique()  # X-axis labels\n",
    "    values1 =  [len(noisy_df[noisy_df[\"clean_label\"] == label]) for label in range(len(categories))] # First set of values\n",
    "    values2 = [len(corrected_df[corrected_df[\"clean_label\"] == label]) for label in range(len(categories))]  # Second set of values\n",
    "\n",
    "    # Define the width of each bar\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Generate positions for the bars\n",
    "    x = np.arange(len(categories))\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.bar(x - bar_width / 2, values1, width=bar_width, color='orange', label='Noisy Labels')\n",
    "    plt.bar(x + bar_width / 2, values2, width=bar_width, color='blue', label='Corrected Labels')\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Ground Truths')\n",
    "    plt.ylabel('Number of Data')\n",
    "    plt.title(f'Total Noisy Labels = {len(noisy_df)}, Total Corrected Labels = {len(corrected_df)}, Correction Ratio = {round(len(corrected_df)/len(noisy_df), 4)}')\n",
    "    plt.xticks(ticks=x, labels=x)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the values on top of the bars\n",
    "    for i, (v1, v2) in enumerate(zip(values1, values2)):\n",
    "        plt.text(x[i] - bar_width / 2, v1 + 0.2, str(v1), ha='center')\n",
    "        plt.text(x[i] + bar_width / 2, v2 + 0.2, str(v2), ha='center')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_plot(df): \n",
    "    # Sigma Analysis\n",
    "    sigma_clean = df[df['mean'].round() == df['clean_label']]['sigma']\n",
    "    sigma_noisy = df[df['mean'].round() != df['clean_label']]['sigma']\n",
    "\n",
    "    # Set fixed number of bins\n",
    "    num_bins = 20\n",
    "    min_val = min(sigma_clean.min(), sigma_noisy.min())\n",
    "    max_val = max(sigma_clean.max(), sigma_noisy.max())\n",
    "    bins = np.linspace(min_val, max_val, num_bins + 1)  # Bin edges\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2  # Bin centers for labels\n",
    "\n",
    "    # Create side-by-side subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "\n",
    "    for ax, sigma_data, color, title in zip(\n",
    "        axes, [sigma_clean, sigma_noisy], ['green', 'red'],\n",
    "        ['Sigma Distribution (Clean Data)', 'Sigma Distribution (Noisy Data)']):\n",
    "\n",
    "        # Create histogram using fixed bins\n",
    "        counts, _, patches = ax.hist(sigma_data, bins=bins, color=color, alpha=0.8, edgecolor='black')\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Sigma')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "        # Rotate x-axis labels (showing bin centers instead of edges)\n",
    "        ax.set_xticks(bin_centers)  \n",
    "        ax.set_xticklabels([f\"{tick:.2f}\" for tick in bin_centers], rotation=90)\n",
    "\n",
    "        # Add value labels on top of bars\n",
    "        for count, patch in zip(counts, patches):\n",
    "            if count > 0:\n",
    "                ax.text(patch.get_x() + patch.get_width()/2, count, int(count), \n",
    "                        ha='center', va='bottom', fontsize=10, color='black', rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_cases(df, n=3, high_sigma=True):\n",
    "    sample = df.sort_values('sigma', ascending=not high_sigma).head(n)\n",
    "    fig, axs = plt.subplots(1, n, figsize=(15, 5))\n",
    "    for i, (idx, row) in enumerate(sample.iterrows()):\n",
    "        img = plt.imread(row['img_path'])\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(f\"Noisy: {row['noisy_label']}\\nClean: {row['clean_label']}\\nMean: {row['mean']}\\nSigma: {row['sigma']:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "    plt.suptitle(f\"Sample {'High' if high_sigma else 'Low'} Sigma Cases\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_confusion(df):\n",
    "    # 1. Confusion Matrix: Noisy vs. Corrected vs. Clean\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    copy_df = copy.deepcopy(df)\n",
    "    # Discretize the `mean` column to match `clean_label`\n",
    "    copy_df['mean_discretized'] = copy_df['mean'].round().astype(int)\n",
    "\n",
    "    # Confusion matrices\n",
    "    confusion_noisy_clean = confusion_matrix(copy_df['noisy_label'],\n",
    "                                            copy_df['clean_label'])\n",
    "    # Compute confusion matrix for corrected means vs. clean labels\n",
    "    confusion_corrected_clean = confusion_matrix(\n",
    "        copy_df['mean_discretized'], \n",
    "        copy_df['clean_label']\n",
    "    )\n",
    "\n",
    "    # Plot confusion matrices\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ConfusionMatrixDisplay(confusion_noisy_clean).plot(ax=ax[0], cmap='Blues')\n",
    "    ax[0].set_title('Noisy Label vs. Clean Label')\n",
    "    ConfusionMatrixDisplay(confusion_corrected_clean).plot(ax=ax[1], cmap='Blues')\n",
    "    ax[1].set_title('Corrected Mean vs. Clean Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_sigma_hist_split(df):\n",
    "    # Group the DataFrame by 'noisy_label'\n",
    "    grouped = df.groupby('noisy_label')\n",
    "    \n",
    "    # Determine the number of unique noisy_labels\n",
    "    num_labels = len(grouped)\n",
    "    \n",
    "    # Create a figure with subplots: one row per noisy_label, four columns\n",
    "    fig, axes = plt.subplots(num_labels, 4, figsize=(20, 5 * num_labels))\n",
    "    \n",
    "    # If there's only one noisy_label, axes will be 1D, so we reshape it for consistency\n",
    "    if num_labels == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Iterate over each noisy_label group\n",
    "    for i, (label, group) in enumerate(grouped):\n",
    "        # Calculate the mean of 'mean_hist' for each row and round it\n",
    "        group['mean_rounded'] = group['mean_hist'].apply(lambda x: round(np.mean(x)))\n",
    "        \n",
    "        # Split the group into two subsets:\n",
    "        # 1. clean_label == round(mean)\n",
    "        # 2. clean_label != round(mean)\n",
    "        equal_mask = group['clean_label'] == group['mean_rounded']\n",
    "        equal_group = group[equal_mask]\n",
    "        not_equal_group = group[~equal_mask]\n",
    "        \n",
    "        # Extract mean_hist and sigma_hist for both subsets\n",
    "        mean_hist_equal = np.array(equal_group['mean_hist'].tolist())\n",
    "        sigma_hist_equal = np.array(equal_group['sigma_hist'].tolist())\n",
    "        \n",
    "        mean_hist_not_equal = np.array(not_equal_group['mean_hist'].tolist())\n",
    "        sigma_hist_not_equal = np.array(not_equal_group['sigma_hist'].tolist())\n",
    "        \n",
    "        # Calculate mean and std for mean_hist and sigma_hist for both subsets\n",
    "        mean_mean_hist_equal = np.mean(mean_hist_equal, axis=0)\n",
    "        std_mean_hist_equal = np.std(mean_hist_equal, axis=0)\n",
    "        \n",
    "        mean_sigma_hist_equal = np.mean(sigma_hist_equal, axis=0)\n",
    "        std_sigma_hist_equal = np.std(sigma_hist_equal, axis=0)\n",
    "        \n",
    "        mean_mean_hist_not_equal = np.mean(mean_hist_not_equal, axis=0)\n",
    "        std_mean_hist_not_equal = np.std(mean_hist_not_equal, axis=0)\n",
    "        \n",
    "        mean_sigma_hist_not_equal = np.mean(sigma_hist_not_equal, axis=0)\n",
    "        std_sigma_hist_not_equal = np.std(sigma_hist_not_equal, axis=0)\n",
    "        \n",
    "        # Generate positions for the x-axis\n",
    "        positions = np.arange(len(mean_mean_hist_equal))\n",
    "        \n",
    "        # Plot for clean_label == round(mean) (left two subplots)\n",
    "        ax = axes[i, 0]\n",
    "        ax.plot(positions, mean_mean_hist_equal, label='Mean', color='blue', linewidth=2)\n",
    "        ax.fill_between(positions, mean_mean_hist_equal - std_mean_hist_equal, mean_mean_hist_equal + std_mean_hist_equal, \n",
    "                        color='blue', alpha=0.3, label='Std Dev')\n",
    "        ax.set_title(f'Mean Hist (clean_label == round(mean), noisy_label = {label})')\n",
    "        ax.set_xlabel('Position')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        ax = axes[i, 1]\n",
    "        ax.plot(positions, mean_sigma_hist_equal, label='Mean', color='green', linewidth=2)\n",
    "        ax.fill_between(positions, mean_sigma_hist_equal - std_sigma_hist_equal, mean_sigma_hist_equal + std_sigma_hist_equal, \n",
    "                        color='green', alpha=0.3, label='Std Dev')\n",
    "        ax.set_title(f'Sigma Hist (clean_label == round(mean), noisy_label = {label})')\n",
    "        ax.set_xlabel('Position')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot for clean_label != round(mean) (right two subplots)\n",
    "        ax = axes[i, 2]\n",
    "        ax.plot(positions, mean_mean_hist_not_equal, label='Mean', color='red', linewidth=2)\n",
    "        ax.fill_between(positions, mean_mean_hist_not_equal - std_mean_hist_not_equal, mean_mean_hist_not_equal + std_mean_hist_not_equal, \n",
    "                        color='red', alpha=0.3, label='Std Dev')\n",
    "        ax.set_title(f'Mean Hist (clean_label != round(mean), noisy_label = {label})')\n",
    "        ax.set_xlabel('Position')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        ax = axes[i, 3]\n",
    "        ax.plot(positions, mean_sigma_hist_not_equal, label='Mean', color='orange', linewidth=2)\n",
    "        ax.fill_between(positions, mean_sigma_hist_not_equal - std_sigma_hist_not_equal, mean_sigma_hist_not_equal + std_sigma_hist_not_equal, \n",
    "                        color='orange', alpha=0.3, label='Std Dev')\n",
    "        ax.set_title(f'Sigma Hist (clean_label != round(mean), noisy_label = {label})')\n",
    "        ax.set_xlabel('Position')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `df` is your DataFrame with columns 'noisy_label', 'clean_label', 'mean_hist', and 'sigma_hist'\n",
    "# plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history_dicts(root):\n",
    "    mean_history_paths = [os.path.join(root, f\"split{split}\", 'mean_history.json') for split in range(5)]\n",
    "    sigma_history_paths = [os.path.join(root, f\"split{split}\", 'sigma_history.json') for split in range(5)]\n",
    "    mean_history = dict()\n",
    "    sigma_history = dict()\n",
    "    for mean_history_path, sigma_history_path in zip(mean_history_paths, sigma_history_paths):\n",
    "        with open(mean_history_path, 'r') as f:\n",
    "            mean_history.update(json.load(f))\n",
    "        with open(sigma_history_path, 'r') as f:\n",
    "            sigma_history.update(json.load(f))\n",
    "    \n",
    "    mean_history = {int(k): v for k, v in mean_history.items()}\n",
    "    sigma_history = {int(k): v for k, v in sigma_history.items()}\n",
    "    \n",
    "    return mean_history, sigma_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(noisy_root, clean_root, result_root=None):\n",
    "    # List of CSV files\n",
    "    files = [os.path.join(noisy_root, f'data_split{i}.csv') for i in range(5)]\n",
    "\n",
    "    # List to store filtered data\n",
    "    filtered_data = []\n",
    "\n",
    "    # Read files and filter data\n",
    "    ids = list()\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, header=None)  # No header in the files\n",
    "        for _, row in df.iterrows():\n",
    "            id = row[0]\n",
    "            folder = row[2]  # Third column is folder\n",
    "            if id in ids or folder == 2:\n",
    "                continue\n",
    "            else:\n",
    "                # if folder in [0, 1]:  # Only include rows where folder is 0 or 1\n",
    "                    # Drop the folder column (index 2) and keep the rest\n",
    "                filtered_row = row.drop(2).tolist()\n",
    "                filtered_data.append(filtered_row)\n",
    "                ids.append(id)\n",
    "\n",
    "    # Create a new DataFrame from the filtered data\n",
    "    # Define column names (assuming the original CSV files have no headers)\n",
    "    columns = [\"id\", \"img_path\", \"noisy_label\", \"mean\", \"sigma\"]  # Adjust column names as needed\n",
    "    df = pd.DataFrame(filtered_data, columns=columns)\n",
    "\n",
    "    clean_file = os.path.join(clean_root, f\"data_split0.csv\")\n",
    "    clean_df = pd.read_csv(clean_file, header=None)\n",
    "    df['clean_label'] = clean_df[3]\n",
    "    \n",
    "    if result_root:\n",
    "        mean_hist, sigma_hist = get_history_dicts(result_root)\n",
    "        df['mean_hist'] = df['id'].map(mean_hist)\n",
    "        df['sigma_hist'] = df['id'].map(sigma_hist)\n",
    "        for i in range(5):\n",
    "            eval_file = os.path.join(result_root, f\"split{i}\", \"evaluation.pt\")\n",
    "            res = torch.load(eval_file)\n",
    "            df[f'predicted_label{i}'] = res['predicted_label']['age']\n",
    "            df[f'pred_proba{i}'] = list(np.array(F.softmax(torch.tensor(res['posterior']['age']), dim=1)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_best',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_best')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_best',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_best')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_skip')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_skip')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_skip')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_skip')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_best_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_best_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_best_skip')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_train_val_best_skip')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_best_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                pred_proba0  \\\n",
      "0     [0.1690077930688858, 0.15907450020313263, 0.11619199067354202, 0.1165652945637703, 0.11709901690483093, 0.10880598425865173, 0.10679558664560318, 0.1064596027135849]   \n",
      "1     [0.1183052584528923, 0.17798162996768951, 0.1287125200033188, 0.12473446130752563, 0.1246262639760971, 0.11109419912099838, 0.10755671560764313, 0.10698913782835007]   \n",
      "2    [0.11241410672664642, 0.13180336356163025, 0.13948747515678406, 0.1465267688035965, 0.1393209844827652, 0.11401525139808655, 0.10868741571903229, 0.10774452239274979]   \n",
      "3    [0.11140300333499908, 0.11988373845815659, 0.11972156912088394, 0.1483953893184662, 0.1672607660293579, 0.11697953194379807, 0.10882211476564407, 0.10753363370895386]   \n",
      "4  [0.10839007794857025, 0.11156205087900162, 0.11264674365520477, 0.13217581808567047, 0.20138566195964813, 0.11892497539520264, 0.10811834782361984, 0.10679584741592407]   \n",
      "5    [0.10965979844331741, 0.1133643090724945, 0.11341147869825363, 0.12576813995838165, 0.18174363672733307, 0.1369628757238388, 0.11144191771745682, 0.10764817893505096]   \n",
      "6     [0.11107051372528076, 0.11444783210754395, 0.1150776594877243, 0.12601909041404724, 0.1598176211118698, 0.14738628268241882, 0.11741295456886292, 0.1087680533528328]   \n",
      "7    [0.11011575162410736, 0.11409943550825119, 0.11410874873399734, 0.12292487919330597, 0.149861678481102, 0.14650338888168335, 0.13035883009433746, 0.11202698945999146]   \n",
      "\n",
      "                                                                                                                                                               pred_proba1  \\\n",
      "0  [0.18410836160182953, 0.1413782835006714, 0.12057548016309738, 0.11568722128868103, 0.11338375508785248, 0.10961102694272995, 0.10784199833869934, 0.10741393268108368]   \n",
      "1   [0.1307082325220108, 0.16267971694469452, 0.13383008539676666, 0.12374123930931091, 0.11884979903697968, 0.1122821569442749, 0.10935010761022568, 0.10855879634618759]   \n",
      "2   [0.11672163009643555, 0.1318739950656891, 0.14471319317817688, 0.14157822728157043, 0.1298060417175293, 0.11563464254140854, 0.11046087741851807, 0.10921145230531693]   \n",
      "3     [0.11380815505981445, 0.1187119409441948, 0.12680010497570038, 0.14649860560894012, 0.152197927236557, 0.1213538646697998, 0.11135007441043854, 0.10927948355674744]   \n",
      "4   [0.1102721244096756, 0.11224344372749329, 0.11706609278917313, 0.13392294943332672, 0.1776290088891983, 0.12807951867580414, 0.11186838895082474, 0.10891857743263245]   \n",
      "5  [0.11096366494894028, 0.11265578120946884, 0.11548186093568802, 0.1250443011522293, 0.15704764425754547, 0.14961029589176178, 0.11889024078845978, 0.11030653119087219]   \n",
      "6   [0.11131177097558975, 0.1131272241473198, 0.11565407365560532, 0.1226731613278389, 0.14020922780036926, 0.15264186263084412, 0.13099391758441925, 0.11338867992162704]   \n",
      "7   [0.11095765978097916, 0.11265675723552704, 0.11482515186071396, 0.1201198548078537, 0.1328478306531906, 0.14369532465934753, 0.14184735715389252, 0.12305013090372086]   \n",
      "\n",
      "                                                                                                                                                                pred_proba2  \\\n",
      "0    [0.17135649919509888, 0.14903084933757782, 0.12205427139997482, 0.1181727722287178, 0.11482471972703934, 0.10947103798389435, 0.1077004224061966, 0.10738910734653473]   \n",
      "1    [0.12222467362880707, 0.16648301482200623, 0.13409896194934845, 0.12667115032672882, 0.1214534342288971, 0.1121969148516655, 0.10877988487482071, 0.10809163004159927]   \n",
      "2    [0.1139613687992096, 0.12905582785606384, 0.1424548476934433, 0.14541897177696228, 0.13480789959430695, 0.11568573862314224, 0.10991891473531723, 0.10869613289833069]   \n",
      "3  [0.11229802668094635, 0.11815739423036575, 0.12396250665187836, 0.15058644115924835, 0.15658774971961975, 0.11975482851266861, 0.11013107746839523, 0.10852208733558655]   \n",
      "4    [0.109101802110672, 0.11162073165178299, 0.11501100659370422, 0.13416512310504913, 0.18926267325878143, 0.12326012551784515, 0.10972399264574051, 0.10785491019487381]   \n",
      "5     [0.11059160530567169, 0.11307132989168167, 0.11523151397705078, 0.12603308260440826, 0.1662268191576004, 0.14502698183059692, 0.114779993891716, 0.10903845727443695]   \n",
      "6   [0.11088467389345169, 0.11364784836769104, 0.11629071086645126, 0.1258641928434372, 0.14943057298660278, 0.14921428263187408, 0.12376303225755692, 0.11090492457151413]   \n",
      "7   [0.11064253002405167, 0.11338959634304047, 0.1151970848441124, 0.12299695611000061, 0.14032381772994995, 0.14548037946224213, 0.13590705394744873, 0.11606255173683167]   \n",
      "\n",
      "                                                                                                                                                                pred_proba3  \\\n",
      "0   [0.1872824728488922, 0.14817287027835846, 0.11262088268995285, 0.11604059487581253, 0.12013570219278336, 0.10650084167718887, 0.10472942143678665, 0.10452200472354889]   \n",
      "1     [0.12390369176864624, 0.17617014050483704, 0.1264188289642334, 0.12222980707883835, 0.13221581280231476, 0.1088605672121048, 0.1052546575665474, 0.10494735836982727]   \n",
      "2   [0.11353712528944016, 0.12958385050296783, 0.1364857256412506, 0.14796695113182068, 0.15143561363220215, 0.11042740195989609, 0.10564178973436356, 0.10492191463708878]   \n",
      "3  [0.11254998296499252, 0.11675640940666199, 0.11431001126766205, 0.14863047003746033, 0.18417178094387054, 0.11355895549058914, 0.10544178634881973, 0.10458043217658997]   \n",
      "4   [0.10771074146032333, 0.10877662897109985, 0.10872051119804382, 0.1311049461364746, 0.21942944824695587, 0.11469291150569916, 0.10519900918006897, 0.10436515510082245]   \n",
      "5     [0.10984774678945541, 0.11051028221845627, 0.1092587485909462, 0.12239038944244385, 0.20108474791049957, 0.1353164166212082, 0.1069251000881195, 0.10466672480106354]   \n",
      "6        [0.1109289824962616, 0.11135529726743698, 0.1110348254442215, 0.12295400351285934, 0.17734295129776, 0.1508091390132904, 0.11027122288942337, 0.10530366748571396]   \n",
      "7    [0.11021070927381516, 0.11145065724849701, 0.11005169153213501, 0.11937045305967331, 0.1624423861503601, 0.15488839149475098, 0.1257961392402649, 0.10578969120979309]   \n",
      "\n",
      "                                                                                                                                                                pred_proba4  \n",
      "0   [0.16536806523799896, 0.1576041579246521, 0.12027457356452942, 0.11647891253232956, 0.11531861871480942, 0.10999709367752075, 0.10771570354700089, 0.10724376887083054]  \n",
      "1   [0.11982693523168564, 0.17064008116722107, 0.13241590559482574, 0.12436637282371521, 0.12322074919939041, 0.11301858723163605, 0.1086522787809372, 0.10785964131355286]  \n",
      "2   [0.11339965462684631, 0.13043943047523499, 0.14300838112831116, 0.1434379667043686, 0.13477081060409546, 0.11653176695108414, 0.10984303057193756, 0.10856910049915314]  \n",
      "3      [0.11228197067975998, 0.11894693970680237, 0.12188825011253357, 0.1490442007780075, 0.1574009358882904, 0.12123997509479523, 0.110577791929245, 0.10862036794424057]  \n",
      "4  [0.10956317186355591, 0.11240426450967789, 0.11492258310317993, 0.13359811902046204, 0.18268141150474548, 0.12746739387512207, 0.11097211390733719, 0.10839106142520905]  \n",
      "5     [0.11080186814069748, 0.11367907375097275, 0.11433816701173782, 0.12517361342906952, 0.16161683201789856, 0.148667573928833, 0.11626763641834259, 0.1094553992152214]  \n",
      "6      [0.1114623174071312, 0.11497538536787033, 0.1154656931757927, 0.12509505450725555, 0.14744415879249573, 0.149595707654953, 0.12453333288431168, 0.11142827570438385]  \n",
      "7   [0.11098091304302216, 0.11406488716602325, 0.11432885378599167, 0.1219504103064537, 0.13824908435344696, 0.14684848487377167, 0.13639560341835022, 0.11718185991048813]  \n"
     ]
    }
   ],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_best_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_1_best_skip')\n",
    "\n",
    "df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_elementwise_means(df):\n",
    "    # Group by 'noisy_label'\n",
    "    grouped = df.groupby('noisy_label')\n",
    "    \n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {}\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for name, group in grouped:\n",
    "        # Extract the columns 'pred_proba0' to 'pred_proba4'\n",
    "        proba_columns = [f'pred_proba{i}' for i in range(5)]\n",
    "        proba_data = group[proba_columns]\n",
    "        \n",
    "        # Calculate the element-wise mean for each column\n",
    "        mean_proba = {}\n",
    "        for col in proba_columns:\n",
    "            # Convert the column of lists into a 2D numpy array\n",
    "            array_of_lists = np.array(group[col].tolist())\n",
    "            # Calculate the mean along axis=0 (element-wise mean)\n",
    "            mean_proba[col] = np.mean(array_of_lists, axis=0).tolist()\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        results[name] = mean_proba\n",
    "    \n",
    "    # Convert the dictionary to a dataframe\n",
    "    result_df = pd.DataFrame(results).T\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your dataframe is named `df`\n",
    "result = calculate_elementwise_means(df)\n",
    "pd.set_option('display.max_colwidth', None) # Show full content of each cell\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.043307\n",
       "1        0.011086\n",
       "2        0.025734\n",
       "3        0.134010\n",
       "4        0.011086\n",
       "           ...   \n",
       "17697    0.019328\n",
       "17698    0.060398\n",
       "17699    0.020366\n",
       "17700    0.035363\n",
       "17701    0.026207\n",
       "Name: sigma, Length: 17702, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sigma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_best_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(noisy_root='facebase/data/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_best_skip',\n",
    "            clean_root='facebase/data/Adience_256x256_resnet50_imagenet_dldl_v2_clean',\n",
    "            result_root='facebase/results/Adience_256x256_resnet50_imagenet_noisy_dldl_v2_sync_2_best_skip')\n",
    "\n",
    "plot_mean_sigma_hist_split(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
